---
title: How to Buy a TV or Monitor For Gaming
summary: "With the PS5/Xbox Series X coming out soon, you might be thinking about buying a new TV or monitor. Unfortunately, display companies make the decision-making process as confusing as possible (likely on purpose). To combat the intentional misinformation, here's a guide that (should) make things a little clearer."
shouldShowTableOfContents: true
shouldShowUpdatedAt: false
createdAt: "2020-11-14T23:41:34.140Z"
titleImage: /couple-tv.jpeg
titleImageCaption: "\"God, the input lag on this TV is awful. I think I want a divorce.\""
tags: 
  - How To
  - Gaming
---

## No Time, Give Me The Quick Version
- If you aren't sure what your setup currently supports, then it likely can't support 4K/120FPS content.
- If you have a 4K TV that's equal or less than 3 years old, then you're probably fine for 4K/60FPS.
- If you're a competitive gamer, buy a monitor (lower input lag).
- Use <a href="https://www.rtings.com/">RTings</a> to find a displays with your desired specs
- Console games will likely not be rendered in true 4K/120hz for a *long* time, so keep your [FOMO](https://en.wikipedia.org/wiki/Fear_of_missing_out) in check.

## Disclaimers
Firstly, I recognize that this article is journalistic low-hanging fruit: there's a million screeching Youtubers and media outlets (i.e. The Verge, Polygon, IGN, etc) that are writing articles like this one. Their write-ups rely on fear and ignorance of the audience to shill expensive and unnecessary upgrades. Ever notice that all of their articles are chock full of amazon referral links ($$$)?

So, fear not: I'm not here to sell you anything. Instead, I will be giving you the tools you need to make informed decisions, and then give you short, concise recommendations based on specific use cases.

Secondly, I want to shout out <a href="https://www.rtings.com/">RTINGS.com</a>. I've sourced some of their images in this article, and in general, I've used their product reviews to help me make informed decisions when buying my own gear. They are independently funded, so if you have the means, throw some support their way.

## Essential concepts
Before making any decision about what to purchase, it's important to understand the basic concepts of display technology so that you can make meaningful comparisons between displays on your own. The main concepts are covered in the subsections below:

### Resolution
Resolution refers to how many pixels are on the screen, often abbreviated to specific measurements like "1080p" or "4k", and sometimes "FHD" and "QHD". Increasing the number of pixels on the screen allows you to discern more detail in any given image. See the chart below illustrating the difference in pixel count between the various resolution categories:

<div class="imageContainer">
  <img class="" :src="'/resolution.png'" />
  <span class="titleImageCaption text--secondary">Comparison between 720p, 1080p, and 4K, taken from <a href="https://www.rtings.com/tv/learn/4k-ultra-hd-uhd-vs-1080p-full-hd-tvs-and-upscaling-compared">RTings</a></span>
</div>

### Refresh Rate (or "framerate")
The refresh rate of a display is the maximum amount of frames-per-second that the display supports, often expressed as "60hz", "120hz", etc. This affects the blurriness of objects in motion. When you have high refresh rates, objects will appear very clear and distinct while in motion. When refresh rates are low, objects will appear blurry and indistinct while in motion. 

To put it another way, think of frame rate like the number of pages in a flipbook: when there are more pages in a flip book for a given animation, the animation will be smoother. When there are *less* pages, then the animation will be choppier. 

<div class="imageContainer small">
  <img class="small" :src="'/flip_book.gif'" />
  <span class="titleImageCaption">This is a flipbook, <a href="https://nycdoe-cs4all.github.io/units/4/learning_activities/la_1">taken from here</a></span>
</div>

The refresh rate of a display functions on the same principal: animations will be smoother when the refresh rate of a monitor is higher, and vice versa.

Note that buying a high refresh-rate display will have little benefit if your games have low framerates. For example, if you only play Nintendo Switch or PS4 games, it makes little sense to buy a 120hz monitor, since both consoles can only output at 60hz.

### Response Time
This marketing point is probably the most-talked-about, yet also the least understood. It does **not** measure how fast the monitor "responds" to your mouse/controller inputs. Instead, it measures how fast a pixel changes colors. In practice, this affects how blurry an object appears as it moves across your screen, which is important for action-oriented games. See the image below, which is taken from <a href="https://www.rtings.com/tv/tests/motion/motion-blur-and-response-time">RTings's excellent article on response time:</a>

<div class="imageContainer">
  <img class="" :src="'/response_time.PNG'" />
</div>

The object in motion is blurrier on the left because of the monitor's worse response time (33.3ms) than the monitor on the right (6.8ms). This metric is usually pretty easy to find in the advertising for a TV or monitor.

### Input Latency (or "Input Lag")
This is the time is takes for the display to respond to your mouse/controller/keyboard\*. This is most important in action-oriented games, but in my opinion, a display with bad input latency feels terrible in almost every use case: working, gaming, chatting, etc.

see below for an example of very high input latency:

<div class="imageContainer">
  <img class="limitedMedium" :src="'/inputLag.gif'" />
  <span class="titleImageCaption text--secondary">Note the delay between when the stick is tilted, and when the character actually moves. That's input lag. <a href="https://www.resetera.com/threads/resolutions-frame-rates-polygon-count-and-load-times-are-all-cool-but-can-we-please-address-input-delay.120575/page-3">Taken from Here</a></span>
</div>

Unfortunately, information about any given display's input latency is difficult to find. Most display manufacturers neglect to include this metric in their marketing, and without expensive tools, it's difficult for an average consumer to measure this for themselves. 

To make matters more confusing, input latency has very weak correlations to price: sometimes, very expensive displays will have high latency, while very cheap ones may have low latency. Therefore, you'll have to rely on independent sources for this information. Fortunately, <a href="https://www.rtings.com/monitor/tests/inputs/input-lag">RTings publishes their input lag test results</a>, so you can check their database before you buy anything.

### HDMI Versions
One final thing you'll need to understand is HDMI support. In order to achieve 4K/120hz, your TV or monitor will need to support HDMI 2.1. See the chart below on how the HDMI versions correlate to resolution and framerate:

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-u0o7{border-color:inherit;font-weight:bold;text-align:left;text-decoration:underline;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-u0o7">HDMI Version</th>
    <th class="tg-u0o7">Max Resolution</th>
    <th class="tg-u0o7">Max Framerate @ 4K</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky">1.4</td>
    <td class="tg-0pky">4K</td>
    <td class="tg-0pky">30</td>
  </tr>
  <tr>
    <td class="tg-0pky">2</td>
    <td class="tg-0pky">4k</td>
    <td class="tg-0pky">60</td>
  </tr>
  <tr>
    <td class="tg-0pky">2.1</td>
    <td class="tg-0pky">10k</td>
    <td class="tg-0pky">120</td>
  </tr>
</tbody>
</table>
<br/>
The implication of this chart is that, if your TV or monitor is 4K, but only supports HDMI 1.4, then that means that you WONT be able to achieve 4K/60FPS. So double check the highest supported HDMI version before buying something, especially if you plan on purchasing the display for a console.

## TV or Monitor?
A major decision you'll need to make is whether you want a TV or monitor. Historically, TVs are usually cheaper since they're more of a commodity, but at the high-end (i.e. 4K 120hz) TV's tend to be *more* expensive than monitors. Regardless, I think the most important factor is your use case:
- Do you need something big to sit in a living room? Or will it go on a table/desk? 
- Do you strictly play console games? Or do you play PC games?
- Do you own a laptop? Or do you own a desktop PC and find use in a standalone monitor?

The one major advantage of a monitor, however, is that they usually have much better input latency than TVs. Your average monitor will have latency somewhere between 8ms and 30ms, while your average consumer TV (i.e. from Best Buy and the like) may have input latency anywhere between 30ms and 100ms.

### I Need a Monitor
If you need a monitor, then you'll need to take note of the HDMI specification chart above, and you'll also need to decide on the type of "display technology" you want. Briefly, they're described below:
- **IPS**: has the best viewing angles and colors. Slower response times than TN panels, and usually more expensive
- **TN**: Fastest response times but colors change the most when viewed at an angle. Usually the cheapest
- **VA**: Somewhere in between IPS and TN. In my opinion, I wouldn't recommend these for anyone.

Note that if you have a somewhat high budget, you can "brute-force" your way to an IPS display that has fast response times and good input latency, so I'd recommend an IPS panel if your budget allows for it. The best way to decide between these three is to simply go to a computer store and check out examples.

## What Do I Do With This Information?
With all of this information, you have a choice to make based on what matters to you (unless you have unlimited money, in which case you can buy LG or Sony's most expensive OLED TV and stop reading). You'll likely need to mix and match the following features based on your needs:
- Higher Resolutions
- Higher Refresh Rates
- Better Response Times
- Low Input Latency
- Supported HDMI Versions
- Extra Features (i.e. additional inputs like DisplayPort, Roku, Spyware Virtual Assistants).

You won't be able to maximize all of these features without spending a ton of cash, so read on for some general recommendations.

## My Recommendations
Armed with the knowledge that I've imparted to you, below you'll find my recommendations on what kind of display to purchase based on common use cases:

**If you know for a fact that you'll be consuming 4K/120FPS content:** purchase a 4K/120FPS monitor or TV with HDMI 2.1. Prices will be around $1000+, regardless of whether you choose a TV or monitor. This is not recommended unless you own a powerful PC, as console games aren't even close to *truly* reaching these rendering specs (despite the marketing push for "4K").

**If you consider yourself a casual gamer who games on either PC or Console**: purchase a 4K/60FPS monitor or TV that supports HDMI 2.0. This should be MOST TVs made within the past 3-4 years, so you may not need to purchase one at all if you already own a 4K TV. Prices for 4K monitors should be fairly reasonable, too.

**If you consider yourself a serious gamer, and own a Series X, PS5, or PC:** buy a 1440p monitor with a high refresh rate, such as 120hz or 144hz. This is the sweetspot in terms of price/performance. 1440p offers clear benefits over 1080p, and the difference between 1440p and 4k is negligible (in my opinion). 1440p TVs don't really exist, and a 4k/120hz TV will likely be too expensive.

## Gotchas
1. **Input latency is lowest at a display's native resolution and refresh rate**. For example, if a monitor's maximum resolution and refresh rate is 1440p/144hz, input lag will likely be higher if outputting at something like 1080p/60hz. This is because the monitor needs to do extra processing to convert the image to a non-native resolution or refresh-rate. So, on your PC or console, ensure that your settings are taking advantage of your display's capabilities

2. **Take notes of the manufacturer's warranty policy.** Buying a TV is pretty easy: you can usually go to a physical store, pick one out, and return it if something goes amiss. But when hunting for a specific gaming monitor, you may have to resort to buying one online. Therefore, when things go wrong, you may be stuck talking directly to the manufacturer. Certain companies like MSI, ASUS, and Razer are an absolute pain to deal with, and some have asinine warranty policies. For example, ASUS believes that selling a monitor with 4 dead pixels is "acceptable", while I happen to think it's "illegal". Therefore, make sure you know what your rights are as a consumer.

3. **Be wary of TV's that claim 120FPS.** There are some TVs out there that CLAIM to support 120FPS content, but don't actually do so. They instead do something called "motion interpolation", otherwise known as the "soap opera effect", which is an algorithm that takes in 60FPS content and, using math and machine learning, inserts new frames into the content based on the differences between each frame. While this is admittedly cool piece of technology, motion interpolation is flawed because the frame it generates is an approximation, and more importantly, **the entire process takes time to compute**. Therefore, it adds a TON of input latency. So to ensure that your TV actually supports 120hz content, check for HDMI 2.1 support.

## A Counterpoint: Will We Actually Get 4K/120FPS Games?
<div class="imageContainer medium">
  <img class="medium" :src="'/craig-the-brute.png'" />
    <span class="titleImageCaption text--secondary">They promised 4K/120FPS and we got...this.</span>
</div>

I need to make one final point before you all go out and by the newest LG ABCD696969 4K3D 120HZ TV. Recall that, during the previous generation (i.e. Xbox One and PS4), we were promised 4K/60FPS, but the vast majority of games never got anywhere *close* to that rendering target.

Then, when the Xbox One X and PS4 Pro came out, both companies were like, "OK, no, for real this time, we gonna do 4K/60FPS", and yet both consoles STILL didn't live up to that claim: they often faked it using techniques like dynamic resolution (i.e. "**up to** 4K") or [checkerboard rendering](https://en.wikipedia.org/wiki/Checkerboard_rendering). Hell, the PS4 Pro's flagship game, God of war, [needed to bump the resolution down](https://www.polygon.com/2018/4/12/17227946/god-of-war-ps4-pro-60fps-4k-1080p) to 1080p so that it could reach 60FPS.

I understand that character models and textures keep increasing in complexity, which eats into the resolution and graphics budget. I also understand that there's marketing pressure to increase image fidelity over image fluidity, as it's hard to convey 120FPS via static screenshots. Consumers want to see the <text-with-tooltip :tooltip="'No, this is not a real metric.'">GigaPoly Bitmapper Shader Units</text-with-tooltip> go *up*, not *down*. Or at least, that's what the marketing department for every single big tech company on the planet thinks, anyway.

Regardless, over-promising and under-delivering is not a new trend in this space, so I advise you to keep your expectations low. Expect games to be rendered **up to** 4K/120hz, but for the majority of them to render far below that for the majority of the time.